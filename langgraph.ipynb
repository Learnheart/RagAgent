{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from pprint import pprint\n",
    "import os\n",
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding & call llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhbk/Documents/python projects/NCKH_2024/RagAgent/agent_libs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_model = FastEmbedEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    cache_dir=\"./embedding_cache\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "llm = ChatGroq(model_name='Llama3-8b-8192', api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chunking text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vectorstore.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/vectorstore.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     chunks_with_metadata = json.load(f)\n\u001b[32m      5\u001b[39m doc_splits = [\n\u001b[32m      6\u001b[39m     Document(\n\u001b[32m      7\u001b[39m         page_content=chunk[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     ) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks_with_metadata\n\u001b[32m     14\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/python projects/NCKH_2024/RagAgent/agent_libs/lib/python3.12/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/vectorstore.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/vectorstore.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks_with_metadata = json.load(f)\n",
    "\n",
    "doc_splits = [\n",
    "    Document(\n",
    "        page_content=chunk[\"text\"],\n",
    "        metadata={\n",
    "            \"chapter\": chunk[\"metadata\"][\"chapter\"],\n",
    "            \"title\": chunk[\"metadata\"][\"title\"],\n",
    "            \"date\": chunk[\"metadata\"][\"date\"]\n",
    "        }\n",
    "    ) for chunk in chunks_with_metadata\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = len(doc_splits)\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save vector db in persist files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = './real_estate_db/luat_dat_dai'\n",
    "persist_directory = './real_estate_db/vectorstore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create db\n",
    "# vectorstore_created = Chroma.from_documents(documents=doc_splits,\n",
    "#                                     embedding=embed_model,\n",
    "#                                     persist_directory=persist_directory,\n",
    "#                                     collection_name=\"vectorstore\")\n",
    "# vectorstore_created.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/64gffy7j1_79tlsdxkhtcjg00000gp/T/ipykernel_97340/1442968809.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed_model, collection_name=\"vectorstore\")\n"
     ]
    }
   ],
   "source": [
    "# call from existed db\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed_model, collection_name=\"vectorstore\")\n",
    "# vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stored documents: 2607\n",
      "Files in persistence directory: ['1aead4fb-bcb3-496d-86d1-dda9b04741f6', 'chroma.sqlite3']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stored documents:\", vectorstore._collection.count())\n",
    "# print(\"First document:\", doc_splits[-1].page_content if doc_splits else \"No documents found!\")\n",
    "print(\"Files in persistence directory:\", os.listdir(persist_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: ['vectorstore']\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "client = PersistentClient(path=persist_directory)\n",
    "collections = client.list_collections()\n",
    "print(\"Available collections:\", collections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_lambda = RunnableLambda(lambda x: retriever.get_relevant_documents(x[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# # convert chunks to document\n",
    "# def format_text_chunks(text_chunks):\n",
    "#     return [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "# retriever_lambda = RunnableLambda(lambda x: format_text_chunks(retriever.get_relevant_documents(x[\"question\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "filter_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an AI-based information filter responsible for categorizing user input questions.\n",
    "    Your mission is to return a binary choice \"yes\" or \"no\" indicating whether the question is related to sensitive topics.\n",
    "    Sensitive topics include hate-speech, sexuality, politics, historical, violence, religion.\n",
    "    \n",
    "    **IMPORTANT**: Your response **MUST** be a valid JSON object with a single key \"score\" and a value of \"yes\" or \"no\". \n",
    "    **NOTE**: If topic is related to vietnamese laws, its not the sensitive topic even refer to senstive topics. \n",
    "    **NOTE**: Some hate-speech often wrote in short cut (\"đcm\", \"mm\", \"vl\") \n",
    "    **DO NOT** include any other text or explanation.\n",
    "    \n",
    "    For example: \n",
    "    Input: \"ai là người lãnh đạo đảng?\"\n",
    "    Output: {{\"score\": \"yes\"}}\n",
    "\n",
    "    Input: \"việt tân là ai?\"\n",
    "    Output: {{\"score\": \"yes\"}}\n",
    "    \n",
    "    Input: \"đcmm\"\n",
    "    Output: {{\"score\": \"yes\"}}\n",
    "    \n",
    "    Input: \"luật việt nam là như nào?\"\n",
    "    Output: {{\"score\": \"no\"}}\n",
    "    \n",
    "    Input: \"tội hiếp dâm và giết người bị phán bao nhiêu năm tù?\"\n",
    "    Output: {{\"score\": \"no\"}}\n",
    "    \n",
    "    Question need to filtered: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "filter_chain = (filter_prompt | llm | JsonOutputParser())\n",
    "\n",
    "# test\n",
    "question = \"“Người sử dụng đất” được hiểu như thế nào theo quy định của Luật Đất đai năm 2024?\"\n",
    "filter_result = filter_chain.invoke({\"question\": question})\n",
    "print(filter_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n",
      "The time required to generate response by Router Chain in seconds:0.24171710014343262\n"
     ]
    }
   ],
   "source": [
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on real estate laws in Vietnam. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "start = time.time()\n",
    "question_router = router_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "print(question_router.invoke({\"question\": question}))\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by Router Chain in seconds:{end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/64gffy7j1_79tlsdxkhtcjg00000gp/T/ipykernel_97340/2309398310.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever_lambda = RunnableLambda(lambda x: retriever.get_relevant_documents(x[\"question\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theo quy định của Luật Đất đai năm 2024, người sử dụng đất được hiểu là các tổ chức, cá nhân có quyền sử dụng đất, bao gồm:\n",
      "\n",
      "* Chủ sở hữu đất đai: là người có quyền sở hữu đất đai theo quy định của pháp luật, bao gồm cá nhân, hộ gia đình, tổ chức và người Việt Nam có tư cách pháp nhân.\n",
      "* Người được giao quyền sử dụng đất: là người được Nhà nước giao quyền sử dụng đất, bao gồm người được giao quyền sử dụng đất theo quyết định của cơ quan nhà nước và người được giao quyền sử dụng đất theo hợp đồng.\n",
      "* Người được thừa kế quyền sử dụng đất: là người được thừa kế quyền sử dụng đất theo quy định của pháp luật.\n",
      "\n",
      "Theo Điều 28 của Luật Đất đai năm 2024, người sử dụng đất phải có quyền sử dụng đất và chịu các nghĩa vụ quản lý, sử dụng đất đai theo quy định của pháp luật.\n",
      "\n",
      "Theo Điều 40 của Nghị định số 96/2024/NĐ-CP, người sử dụng đất phải cung cấp thông tin về quyền sử dụng đất, bao gồm quyền sử dụng đất, hạn chế về quyền sử dụng đất (nếu có), tiến độ thực hiện và các nội dung khác.\n",
      "\n",
      "Căn cứ vào các quy định trên, có thể thấy rằng người sử dụng đất là các đối tượng có quyền sử dụng đất, bao gồm chủ sở hữu đất đai, người được giao quyền sử dụng đất và người được thừa kế quyền sử dụng đất, và người sử dụng đất phải chịu các nghĩa vụ quản lý, sử dụng đất đai theo quy định của pháp luật.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just refuse answer in polite and friendly. \n",
    "    Answer question in detailed, make sure references vietnamese's law that prove for your answer. \n",
    "    For example:\n",
    "    Question: Phạm vi điều chỉnh và đối tượng áp dụng Luật Đất đai năm 2024 là gì?\n",
    "    Answer: \n",
    "    Điều 1. Phạm vi điều chỉnh\n",
    "    Luật này quy định về chế độ sở hữu đất đai, quyền hạn và trách nhiệm của Nhà nước đại diện chủ sở hữu toàn dân về đất đai và thống nhất quản lý về đất đai, chế độ quản lý và sử dụng đất đai, quyền và nghĩa vụ của công dân, người sử dụng đất đối với đất đai thuộc lãnh thổ của nước Cộng hòa xã hội chủ nghĩa Việt Nam.\n",
    "    Điều 2. Đối tượng áp dụng\n",
    "    1. Cơ quan nhà nước thực hiện quyền hạn và trách nhiệm đại diện chủ sở hữu toàn dân về đất đai, thực hiện nhiệm vụ thống nhất quản lý nhà nước về đất đai.\n",
    "    2. Người sử dụng đất.\n",
    "    3. Các đối tượng khác có liên quan đến việc quản lý, sử dụng đất đai.'\n",
    "    \n",
    "    **IMPORTANT**: Your response **MUST** be in Vietnamese, the tone have to professional and polite\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "start = time.time()\n",
    "rag_chain = (\n",
    "    {\"question\": lambda x: x[\"question\"], \"context\": retriever_lambda}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# test\n",
    "# question = \"luật nhà ở 2024\"\n",
    "response = rag_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 content: Điều 28.Nhận quyền sử dụng đất\n",
      "1\n",
      "retriever 1 grade: {'score': 'yes'}\n",
      "doc 2 content: - Những hạn chế về quyền sử dụng đất (nếu có): ..........................................................\n",
      "2\n",
      "retriever 2 grade: {'score': 'yes'}\n",
      "doc 3 content: quyền sử dụng đất thì không cần mô tả thông tin này)\n",
      "- Tiến độ thực hiện: .....................................................................................................\n",
      "- Các nội dung khác: ...................................................................................................\n",
      "điều 2\n",
      "retriever 3 grade: {'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "retrieval_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "start = time.time()\n",
    "retrieval_grader = retrieval_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "docs = retriever.invoke(question)\n",
    "end = time.time()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_txt = doc.page_content\n",
    "    print(f\"doc {i + 1} content: {doc_txt}\")\n",
    "    doc_grader = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "    print(f\"retriever {i + 1} grade: {doc_grader}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time required to generate response by the generation chain in seconds:0.34575486183166504\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "hallucination_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "start = time.time()\n",
    "hallucination_grader = hallucination_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "hallucination_grader_response = hallucination_grader.invoke({\"documents\": docs, \"generation\": response})\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by the generation chain in seconds:{end - start}\")\n",
    "print(hallucination_grader_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANswer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time required to generate response by the answer grader in seconds:0.35233521461486816\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "start = time.time()\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "answer_grader_response = answer_grader.invoke({\"question\": question,\"generation\": response})\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by the answer grader in seconds:{end - start}\")\n",
    "print(answer_grader_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Websearch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_api_key = os.environ['TAVILY_API_KEY']\n",
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single test web_search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Dự báo bất động sản 2024 - CafeLand.Vn', 'url': 'https://cafeland.vn/chu-de-nong/du-bao-bat-dong-san-2024-3101/', 'content': 'Theo dự báo của chuyên gia, năm 2024, ngành bất động sản sẽ đón nhận sự quay trở lại của khoảng 30%-40% môi giới bất động sản. Sự phục hồi rõ nét của thị trường sẽ được thể hiện từ cuối quý 3.2024....\\n\\nNgành bất động sản trong năm 2024 sẽ diễn biến theo xu hướng nào?\\n\\nTái cấu trúc nguồn vốn, M&A dự án và tập trung vào phân khúc trung cấp dự báo sẽ là những xu hướng chính của ngành Bất động sản trong năm 2024. Cụ thể ra sao, các bạn cùng theo dõi tiếp video này nhé.... [...] Thị trường bất động sản được dự báo sẽ có sự chuyển mình trong năm 2024, với hai phân khúc dẫn dắt. Tuy nhiên, khó khăn vẫn đang tiếp diễn, đòi hỏi doanh nghiệp phải nhanh nhạy nắm bắt thời cơ, dồn lực vào các dự án khả thi đồng thời có chính sách li...\\n\\nKịch bản xấu và nguy hiểm cho thị trường bất động sản vẫn có thể xảy ra [...] Mặc dù tình hình kinh tế chung không khả quan, thị trường bất động sản cũng ghi nhận sự khó khăn, nhưng nền giá vẫn tương đối cao so với giá trị thực lẫn khả năng tài chính của người dân. Đặc biệt, giá bán căn h...\\n\\nKịch bản nào cho thị trường đất nền năm 2024?\\n\\nThị trường đóng băng đã tác động mạnh đến nhu cầu và tâm lý người mua đất. Tình trạng này được dự báo sẽ tồn tại xuyên suốt năm 2024, ảnh hưởng đến mặt bằng giá toàn thị trường.', 'score': 0.79099923}, {'title': 'TỔNG HỢP THỊ TRƯỜNG BẤT ĐỘNG SẢN 2024 - HƯỚNG TỚI 2025', 'url': 'https://www.youtube.com/watch?v=cavyT74iC6s', 'content': 'Bộ Xây dựng vừa công bố thông tin về nhà ở và thị trường bất động sản quý IV/2024 và cả năm 2024. Theo đó, thị trường bất động sản 2024 ghi nhận nhiều diễn biến khả quan khi nguồn cung tăng mạnh so với các năm trước, lượng giao dịch có phần được cải thiện và tồn kho bất động sản đang có xu hướng giảm dần. Ngoài ra, dư nợ tín dụng kinh doanh bất động sản đất nền cũng tăng mạnh so với cuối năm 2023. Vậy, hay cùng nhìn lại các số liệu của thị trường bất động sản 2024 và hướng tới năm 2025 cùng', 'score': 0.7771143}, {'title': 'Thị trường bất động sản cuối năm 2024: Kỳ vọng bứt phá', 'url': 'https://consosukien.vn/thi-truong-bat-dong-san-cuoi-nam-2024-ky-vong-but-pha.htm', 'content': 'Cụ thể, mức độ quan tâm đất trong Quý III/2024 dự kiến tăng 49% so với cùng kỳ năm 2023, nhà riêng tăng 25%, chung cư tăng 24%, biệt thự tăng 22', 'score': 0.77364457}, {'title': 'Nhìn lại thị trường bất động sản năm 2024 - Báo Kinh tế đô thị', 'url': 'https://kinhtedothi.vn/nhin-lai-thi-truong-bat-dong-san-nam-2024.html', 'content': 'Tại thời điểm cuối năm 2024, thị trường ghi nhận khoảng 56.000 sản phẩm chào bán trên thị trường sơ cấp, tương đương với thời điểm cuối năm 2023', 'score': 0.76385427}, {'title': 'Năm 2024, nguồn cung bất động sản tăng trưởng mạnh', 'url': 'https://baochinhphu.vn/nam-2024-nguon-cung-bat-dong-san-tang-truong-manh-10224123109473222.htm', 'content': 'Tính chung cả năm 2024, toàn thị trường ghi nhận khoảng gần 81 nghìn sản phẩm chào bán, tăng hơn 40% so với năm 2023. Trong đó, có 65,376 sản', 'score': 0.7402687}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.tools import TavilySearchResults\n",
    "\n",
    "def search_web(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Searches the web using Tavily API for the given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search query.\n",
    "    - k (int): Number of search results to return (default is 3).\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of search result snippets.\n",
    "    \"\"\"\n",
    "    tavily_api_key = os.environ['TAVILY_API_KEY']\n",
    "    web_search_tool = TavilySearchResults(k=k)\n",
    "    \n",
    "    try:\n",
    "        results = web_search_tool.run(query)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error while searching: {e}\")\n",
    "        return []\n",
    "\n",
    "query = \"TÌnh hình bất đọngo sản 2024\"\n",
    "search_results = search_web(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraphh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_question(state):\n",
    "    question = state[\"question\"]\n",
    "    return {\"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    try:\n",
    "        documents = retriever.invoke(question)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval: {e}\")\n",
    "        documents = []\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "#\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    iterations = state.get(\"iterations\", 0) + 1\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"iterations\": iterations}\n",
    "#\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "#\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    iterations = state.get(\"iterations\", 0) + 1\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.run({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question, \"iterations\": iterations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_question(state):\n",
    "    \"\"\"\n",
    "    Filter the question for sensitive topics\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        dict: Updated state with decision to end or continue\n",
    "    \"\"\"\n",
    "    print(\"---FILTER QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    filter_result = filter_chain.invoke({\"question\": question})\n",
    "    if filter_result[\"score\"] == \"yes\":\n",
    "        print(\"---SENSITIVE TOPIC DETECTED---\")\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        print(\"---QUESTION PASSED FILTER---\")\n",
    "        return \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    print(f\"number of iteration at this hallucination term: {iterations}\")\n",
    "    \n",
    "    if iterations >= 5:\n",
    "        print(\"GETTING MAX ATTEMPTS\")\n",
    "        return \"end\"\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x17e0a8aa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"route_question\", route_question)\n",
    "workflow.add_node(\"parse_question\", parse_question)\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry & End points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x17e0a8aa0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_conditional_entry_point(\n",
    "    filter_question,\n",
    "    {\n",
    "        \"yes\": END,\n",
    "        \"no\": \"parse_question\"\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"parse_question\",\n",
    "    route_question,     \n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "        \"end\": END\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FILTER QUESTION---\n",
      "---QUESTION PASSED FILTER---\n",
      "---ROUTE QUESTION---\n",
      "what is my name?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "'Finished running: parse_question:'\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "number of iteration at this hallucination term: 2\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "'Finished running: generate:'\n",
      "(\"I'm happy to help! However, I must politely point out that the context you \"\n",
      " 'provided does not include any information about your name. The documents you '\n",
      " 'shared are related to various Vietnamese laws, but they do not contain any '\n",
      " 'personal information about you or your name.\\n'\n",
      " '\\n'\n",
      " \"As an assistant, I'm designed to provide accurate and reliable information \"\n",
      " 'based on the context provided. In this case, I do not have any information '\n",
      " 'about your name, and I cannot provide an answer to this question.\\n'\n",
      " '\\n'\n",
      " \"If you could provide more context or clarify what you are referring to, I'll \"\n",
      " 'do my best to assist you.')\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "number of iteration at this hallucination term: 3\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "'Finished running: generate:'\n",
      "(\"I apologize, but I don't have the necessary information to answer your \"\n",
      " 'question \"what is my name?\" as there is no context provided about your name. '\n",
      " 'The given context appears to be a collection of documents related to '\n",
      " 'Vietnamese laws, but it does not contain any information about your personal '\n",
      " 'identity.\\n'\n",
      " '\\n'\n",
      " 'If you meant to ask a different question, please feel free to rephrase or '\n",
      " \"provide more context, and I'll do my best to assist you.\")\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "number of iteration at this hallucination term: 4\n",
      "'---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'\n",
      "'Finished running: generate:'\n",
      "('I cannot provide an answer to your question because the provided context '\n",
      " 'does not mention your name.')\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "number of iteration at this hallucination term: 5\n",
      "GETTING MAX ATTEMPTS\n",
      "'Finished running: generate:'\n",
      "(\"I'm happy to help! However, I must politely point out that the context \"\n",
      " 'provided does not contain any information about your name. The documents '\n",
      " \"you've shared are related to Vietnamese laws, including the Law on Business \"\n",
      " 'of Real Estate, the Law on Housing, and the Law on Land, but none of them '\n",
      " 'contain your name.\\n'\n",
      " '\\n'\n",
      " 'As a result, I cannot provide an answer to your question as I do not have '\n",
      " 'any information about your name. If you could provide more context or '\n",
      " \"clarify who you are, I'll be happy to try and assist you further.\")\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"what is my name?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "\n",
    "        if key == \"filter_question\" and value == \"yes\":\n",
    "            pprint(\"xin loi toi khong the tra loi cau hoi nay\")\n",
    "            break\n",
    "\n",
    "    if \"generation\" in value:\n",
    "        pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(input):\n",
    "    question  = {\"question\": input}\n",
    "    for output in app.stream(question):\n",
    "        for key, value in output.items():\n",
    "            if key == \"filter_question\" and value == \"yes\":\n",
    "                return \"Xin lỗi, mình không thể trả lời câu hỏi này vì nó chứa nội dung liên quan đến chủ đề nhạy cảm.\"\n",
    "\n",
    "        if \"generation\" in value:\n",
    "            return value[\"generation\"]\n",
    "    return \"Xin lỗi, mình không có câu trả lời cho câu hỏi của bạn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FILTER QUESTION---\n",
      "---SENSITIVE TOPIC DETECTED---\n",
      "Xin lỗi, mình không có câu trả lời cho câu hỏi của bạn\n"
     ]
    }
   ],
   "source": [
    "input = \"mẹ m\"\n",
    "print(answer(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_libs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
