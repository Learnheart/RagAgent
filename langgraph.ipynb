{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from data_processing import process_and_chunk_data, chunk_single_file\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from pprint import pprint\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding & call llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastembed in e:\\python projects\\ragagent\\agent\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (0.27.1)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (0.7.3)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.26 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (2.2.1)\n",
      "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (1.20.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (10.4.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (0.1.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (0.21.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from fastembed) (4.67.1)\n",
      "Requirement already satisfied: filelock in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
      "Requirement already satisfied: colorama>=0.3.4 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (1.2.0)\n",
      "Requirement already satisfied: coloredlogs in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (24.12.23)\n",
      "Requirement already satisfied: protobuf in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (5.29.3)\n",
      "Requirement already satisfied: sympy in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (2024.12.14)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in e:\\python projects\\ragagent\\agent\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "llm = ChatGroq(model_name='Llama3-8b-8192', api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000012105BAA870>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000012105BB1CD0>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chunking text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Tiêu đề: LUẬT Chương I: QUY ĐỊNH CHUNG Điều 1. Phạm vi điều chỉnh Luật này quy định về chế độ sở hữu đất đai, quyền hạn và trách nhiệm của Nhà nước đại diện chủ sở hữu toàn dân về đất đai và thống nhất quản lý về đất đai, chế độ quản lý và sử dụng đất đai, quyền và nghĩa vụ của công dân, người sử dụng đất đối với đất đai thuộc lãnh thổ của nước Cộng hòa xã hội chủ nghĩa Việt Nam. Điều 2. Đối tượng áp dụng 1. Cơ quan nhà nước thực hiện quyền hạn và trách nhiệm đại diện chủ sở hữu toàn dân về đất')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_texts = process_and_chunk_data('data')\n",
    "raw_texts = chunk_single_file('data/luat_dat_dai.json')\n",
    "doc_splits = [Document(page_content=chunk) for chunk in raw_texts]\n",
    "doc_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11318"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = len(doc_splits)\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save vector db in persist files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'real_estate_db/luat_dat_dai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create db\n",
    "vectorstore_created = Chroma.from_documents(documents=doc_splits,\n",
    "                                    embedding=embed_model,\n",
    "                                    persist_directory=persist_directory,\n",
    "                                    collection_name=\"local-rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_created.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call from existed db\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stored documents: 0\n",
      "First document: Tiêu đề: LUẬT Chương I: QUY ĐỊNH CHUNG Điều 1. Phạm vi điều chỉnh Luật này quy định về chế độ sở hữu đất đai, quyền hạn và trách nhiệm của Nhà nước đại diện chủ sở hữu toàn dân về đất đai và thống nhất quản lý về đất đai, chế độ quản lý và sử dụng đất đai, quyền và nghĩa vụ của công dân, người sử dụng đất đối với đất đai thuộc lãnh thổ của nước Cộng hòa xã hội chủ nghĩa Việt Nam. Điều 2. Đối tượng áp dụng 1. Cơ quan nhà nước thực hiện quyền hạn và trách nhiệm đại diện chủ sở hữu toàn dân về đất\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stored documents:\", vectorstore._collection.count())\n",
    "print(\"First document:\", doc_splits[0].page_content if doc_splits else \"No documents found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_lambda = RunnableLambda(lambda x: retriever.get_relevant_documents(x[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# # convert chunks to document\n",
    "# def format_text_chunks(text_chunks):\n",
    "#     return [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "# retriever_lambda = RunnableLambda(lambda x: format_text_chunks(retriever.get_relevant_documents(x[\"question\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n",
      "The time required to generate response by Router Chain in seconds:0.3053898811340332\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore for questions on Land law and housing law in Vietnam. You do not need to be stringent with the keywords \n",
    "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search' \n",
    "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and \n",
    "    no premable or explaination. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "start = time.time()\n",
    "question_router = router_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "question = \"người dân có quyền sử dụng đất bao nhiêu năm?\"\n",
    "print(question_router.invoke({\"question\": question}))\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by Router Chain in seconds:{end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theo Luật Đất đai 2013, người dân có quyền sử dụng đất ổn định trong 50 năm. Sau 50 năm, người dân có thể được cấp giấy chứng nhận quyền sử dụng đất lâu dài. Tùy vào từng trường hợp, người dân có thể được quyền sử dụng đất lâu dài hơn.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just refuse answer in polite and friendly. \n",
    "    Use three sentences maximum and keep the answer concise and answer using Vietnamese <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Chain\n",
    "start = time.time()\n",
    "rag_chain = (\n",
    "    {\"question\": lambda x: x[\"question\"], \"context\": retriever_lambda}\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# test\n",
    "# question = \"luật nhà ở 2024\"\n",
    "response = rag_chain.invoke({\"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# question = \"agent memory\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39minvoke(question)\n\u001b[1;32m---> 19\u001b[0m doc_txt \u001b[38;5;241m=\u001b[39m \u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(retrieval_grader\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc_txt}))\n\u001b[0;32m     21\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "retrieval_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "start = time.time()\n",
    "retrieval_grader = retrieval_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "# question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by the retrieval grader in seconds:{end - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time required to generate response by the generation chain in seconds:0.5395851135253906\n",
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "hallucination_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "start = time.time()\n",
    "hallucination_grader = hallucination_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "hallucination_grader_response = hallucination_grader.invoke({\"documents\": docs, \"generation\": response})\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by the generation chain in seconds:{end - start}\")\n",
    "print(hallucination_grader_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANswer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time required to generate response by the answer grader in seconds:0.23734021186828613\n",
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "start = time.time()\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# test\n",
    "answer_grader_response = answer_grader.invoke({\"question\": question,\"generation\": response})\n",
    "end = time.time()\n",
    "print(f\"The time required to generate response by the answer grader in seconds:{end - start}\")\n",
    "print(answer_grader_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Websearch tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_api_key = os.environ['TAVILY_API_KEY']\n",
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single test web_search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://kinhtedothi.vn/nhin-lai-thi-truong-bat-dong-san-nam-2024.html', 'content': 'Tính chung cả năm 2024, toàn thị trường ghi nhận khoảng gần 81.000 sản phẩm chào bán, tăng hơn 40% so với năm 2023. Trong đó, có 65,376 sản phẩ'}, {'url': 'https://baochinhphu.vn/chuyen-dong-tich-cuc-cua-thi-truong-bat-dong-san-trong-quy-iii-2024-102241015110847106.htm', 'content': 'Đại diện Viện nghiên cứu Đánh giá thị trường bất động sản Việt Nam cho biết, quý 3 năm 2024, thị trường BĐS nhà ở tiếp tục ghi nhận nguồn cung đạt mức 22.412 sản phẩm được chào bán trên thị trường, với khoảng 14.750 sản phẩm mở bán mới, giảm 25% so với quý trước, nhưng đã tăng 60% so với cùng kỳ năm 2023. Nghiên cứu về chỉ số giá căn hộ, phản ánh mức biến động giá bán bình quân của các dự án trong tập mẫu 150 dự án được Hội Môi giới bất động sản Việt Nam (VARS) chọn lọc và quan sát cũng cho thấy, tính đến quý 3/2024, giá bán trung bình của cụm mẫu dự án ở thành phố Hà Nội gần tiệm cận mức 60 triệu VNĐ/m2, tăng\\xa0 64,0% so với quý 2/2019.'}, {'url': 'https://cafeland.vn/chu-de-nong/du-bao-bat-dong-san-2024-3101/', 'content': 'Cập nhật: 10/03/2024 11:23 AMVới những lợi thế về hạ tầng, thị trường bất động sản tại đây được dự báo sẽ khởi sắc trong năm 2024, đặc biệt, 2 quý đầu năm được xem là thời điểm nhà đầu tư có thể bắt đáy. Cập nhật: 31/12/2023 4:14 PMBất chấp các thách thức về kinh tế và trở ngại chính sách, thị trường bất động sản châu Á – Thái Bình Dương (APAC) vẫn có nhiều điểm sáng trong năm 2024, với nguồn cung bất động sản hậu cần được dự báo tăng gần 50% trong khi giá nhà ở cao cấp có thể ...'}, {'url': 'https://consosukien.vn/thi-truong-bat-dong-san-cuoi-nam-2024-ky-vong-but-pha.htm', 'content': 'Các chuyên gia của VARS nhận định, cùng với đà phục của thị trường,\\u200b các chủ thể tham gia thị trường sẽ bắt đầu tăng tốc gia nhập, xúc tiến các kế hoạch kinh doanh: Các doanh nghiệp phát triển dự án đẩy mạnh triển khai, phối hợp với cơ quan quản lý Nhà nước để giải quyết những \"nút thắt\"; các chủ đầu tư sẽ chủ động hơn trong việc triển khai các dự án mới, và các nhà đầu tư sẽ được tiếp thêm niềm tin, tạo điều kiện cho dòng tiền từ các khoản đáo hạn ngân hàng tiếp tục đổ vào BĐS./.'}, {'url': 'https://dangcongsan.vn/kinh-te/du-bao-nua-cuoi-2024-thi-truong-bat-dong-san-tiep-tuc-phuc-hoi-ben-vung-674990.html', 'content': 'Việc Luật Đất đai năm 2024, Luật Nhà ở năm 2023, Luật Kinh doanh bất động sản năm 2023 và khoản 2, điều 209 của Luật các tổ chức tín dụng sẽ có hiệu lực từ ngày 1/8/2024, sớm hơn năm tháng so với quyết định trước đó, chắc chắn sẽ có tác động tích cực, góp phần thúc đẩy sự phục hồi và phát triển của thị trường bất động sản (BĐS). Nguồn cung trong nửa cuối năm 2024 sẽ tiếp tục được cải thiện, ước tính tăng khoảng 20% so với 06 tháng đầu năm 2024, được đóng góp chủ yếu từ phân khúc căn hộ cao cấp, hạng sang với chất lượng sản phẩm và mức giá bán phục hồi rõ nét cùng với nhiều hơn các sản phẩm thấp tầng khi các đại dự án đang hoàn tất các khâu cuối cùng gia nhập thị trường.'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.tools import TavilySearchResults\n",
    "\n",
    "def search_web(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Searches the web using Tavily API for the given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The search query.\n",
    "    - k (int): Number of search results to return (default is 3).\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of search result snippets.\n",
    "    \"\"\"\n",
    "    tavily_api_key = os.environ['TAVILY_API_KEY']\n",
    "    web_search_tool = TavilySearchResults(k=k)\n",
    "    \n",
    "    try:\n",
    "        results = web_search_tool.run(query)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error while searching: {e}\")\n",
    "        return []\n",
    "\n",
    "query = \"TÌnh hình bất đọngo sản 2024\"\n",
    "search_results = search_web(query)\n",
    "print(search_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraphh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "#\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    iterations = state.get(\"iterations\", 0) + 1\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation, \"iterations\": iterations}\n",
    "#\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "#\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    iterations = state.get(\"iterations\", 0) + 1\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.run({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question, \"iterations\": iterations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    if iterations >= 10:\n",
    "        print(\"GETTING MAX ATTEMPTS\")\n",
    "        return \"end\"\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c791d2fda0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry & End points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c791d2fda0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "        \"end\": END\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "Tôi là ai?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Tôi không thể trả lời câu hỏi \"Tôi là ai?\" vì không có thông tin liên quan '\n",
      " 'đến người nào trong đoạn văn. Xin lỗi!')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"Tôi là ai?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "    # if \"generation\" in value:\n",
    "    #     pprint(value[\"generation\"])\n",
    "    # else:\n",
    "    #     pprint(\"Hiện mình chưa có thông tin về câu hỏi của bạn. Bạn có thể thử lại với câu hỏi khác không?\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
